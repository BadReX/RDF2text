
#Inside the folders (unseenScore\seenScore)  you will find all scores for unseen\seen dataset  for all models, where score files generated automaticaly when you execute the evaluation script(X.sh) for unseen\seen.
#Then to get table latex, just run the script (generateLxtable.py), where it located in the folders (unseenScore\seenScore)sequentially to obtain scores for unseen\seen  for all the models



###################################################################################
Evaluation system steps:
##The first step, we create for each model we have sperate folder, which contains two folder for seen, unseen. Where in each one we put its references files, and extracted predictions file. E.g. flat->seen->dev.ref1,dev.ref2, dev.ref3, flat-seen_predictions.txt


#1- Extract predictions for all (flat,str1,str2):
 python3 predFinder.py -i <file-directory> -f fileName
file-directory is the directory where the file we want to extract predictions from
fileName is the name of generated file with extracted predictions 
Where predFinder.py will generate new folder (ExtractedPred) and put all extracetd predictions in it.


$ python3 predFinder.py -i preds/flat_seen.pred -f flat-seen

$ python3 predFinder.py -i preds/flat_unseen.pred -f flat-unseen

$ python3 predFinder.py -i preds/str1_seen.pred -f str1-seen

$ python3 predFinder.py -i preds/str1_unseen.pred -f str1-unseen

$ python3 predFinder.py -i preds/str2_unseen.pred -f str2-unseen

$ python3 predFinder.py -i preds/str2_seen.pred -f str2-seen

#2- run metrics.py to prepare files for Meteor, Ter

 python3 metrics.py -i <referenceDirectory> -d <dataType> -p <directoryPred>
referenceDirectory: directory where all references are, ending with '/'
dataType: the type of evaluated data, i.e. here its either dev or test
directoryPred: the path for prediction file, for current evaluated data

#Example

$ python3 metrics.py -i flatSeen/ -d dev -p ExtractedPred/flat-seen_predictions.txt
$ python3 metrics.py -i flatUnseen/ -d test -p ExtractedPred/flat-unseen_predictions.txt

$ python3 metrics.py -i str1Seen/ -d dev -p ExtractedPred/str1-seen_predictions.txt
$ python3 metrics.py -i str1Unseen/ -d test -p ExtractedPred/str1-unseen_predictions.txt

$ python3 metrics.py -i str2Seen/ -d dev -p ExtractedPred/str2-seen_predictions.txt
$ python3 metrics.py -i str2Unseen/ -d test -p ExtractedPred/str2-unseen_predictions.txt

#3- run the script (Xevaluat_metrics.sh) to get the evaluation scores for Bleu,Ter,Meteor
## For this step we put the corresponding evaluation script (Xevaluat_metrics.sh) for each data type (i.e. seen or unseen) for each model in its folder, with the script for BLEU,multi-bleu.perl e.g. flatSeen-> flatSeen_evaluat_metrics.sh, multi-bleu.perl with all reference files, predictions .etc
## Also, we put the folder of meteor and Ter in the same directory where flat, str1, str2 folders are.

# Example
# in flatSeen folder
$ ./flatSeen_evaluat_metrics.sh > ../seenScore/flatseen-score.txt
#in flatUnseen folder
$ ./flatUnseen_evaluat_metrics.sh > ../unseenScore/flatunseen-score.txt
# in str1Seen folder
$ ./str1Seen_evaluat_metrics.sh > ../seenScore/str1seen-score.txt
#in str1Unseen folder
$ ./str1Unseen_evaluat_metrics.sh > ../unseenScore/str1unseen-score.txt
# in str2Seen folder
$ ./str2Seen_evaluat_metrics.sh > ../seenScore/str2seen-score.txt
#in str2Unseen folder
$ ./str2Unseen_evaluat_metrics.sh > ../unseenScore/str2unseen-score.txt



##############################################################

# Input data with different category name 

#1- Extract predictions for each category
$ python3 predFinderCategory.py -i <pred directory> -f categoryName
Example
 python3 predFinderCategory.py -i preds/flat_seen.pred -f Astronaut

 python3 predFinderCategory.py -i preds/flat_seen.pred -f City

 python3 predFinderCategory.py -i preds/flat_seen.pred -f University

 python3 predFinderCategory.py -i preds/flat_seen.pred -f Food

 python3 predFinderCategory.py -i preds/flat_seen.pred -f SportsTeam


#2- run metrics.py to prepare files for Meteor, Ter

 python3 metrics.py -i referenceDirectory -d dataType -p directoryPred

dataType here is category name, .i.e. its values are:Astronaut, Food, SportsTeam, City, University.
referenceDirectory, directoryPred: the both are in the same folder triple1, which generated by the script predFinderTriple.py

##Example
python3 metrics.py -i Astronaut/ -d Astronaut -p Astronaut/Astronaut_predictions.txt

 python3 metrics.py -i University/ -d University -p University/University_predictions.txt

 python3 metrics.py -i Food/ -d Food -p Food/Food_predictions.txt

 python3 metrics.py -i SportsTeam/ -d SportsTeam -p SportsTeam/SportsTeam_predictions.txt

 python3 metrics.py -i City/ -d City -p City/City_predictions.txt


#3- run the script (Xevaluat_metrics.sh) to get the evaluation scores for Bleu,Ter,Meteor
## For this step we put in each category folder the two scripts: the corresponding (Xevaluat_metrics.sh), multi-bleu.perl e.g.in Astronaut-> evaluat_metrics.sh, multi-bleu.perl with all reference files, predictions .etc
## Also, we put the folder of meteor and Ter in the same directory where Astronaut, City .etc folders are.

# Example
 ./Astronaut_evaluat_metrics.sh > Astronaut_score.txt
 ./City_evaluat_metrics.sh > City_score.txt
 ./Food_evaluat_metrics.sh > Food_score.txt
 ./SportsTeam_evaluat_metrics.sh > SportsTeam_score.txt
 ./University_evaluat_metrics.sh > University_score.txt


